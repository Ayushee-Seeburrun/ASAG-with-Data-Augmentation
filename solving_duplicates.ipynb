{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOa1b+mWKOIeUaFoZpC1Sa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayushee-Seeburrun/ASAG-with-Data-Augmentation/blob/main/solving_duplicates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SCAMaP_BZe1",
        "outputId": "41d30452-3992-49d1-970e-81d402888fa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "8kin_mMuBoXH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finaldataset = pd.read_csv(\"/content/drive/MyDrive/Data Augmentation/Dataset Splits/train_balanced_s2_s3_final.csv\")\n",
        "print(\"rows before removing duplicate score-3 rows: \", len(finaldataset))\n",
        "print(finaldataset[\"Score2\"].value_counts().sort_index())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWnhZS72Aznc",
        "outputId": "8a4646af-a191-4e0e-b52c-0f6128ff5613"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rows before removing duplicate score-3 rows:  18247\n",
            "Score2\n",
            "0    5424\n",
            "1    4518\n",
            "2    4225\n",
            "3    4080\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "finaldataset_3 = finaldataset[finaldataset[\"Score2\"].isin([3])]\n",
        "\n",
        "total_withdupes = finaldataset_3[finaldataset_3.duplicated(subset=[\"Score2\", \"EssayText\"], keep=False)]\n",
        "\n",
        "duplicates_3 = finaldataset_3.duplicated(subset=[\"Score2\", \"EssayText\"], keep=\"first\").sum()\n",
        "print(\"Total rows including their duplicates(in Score 3): \", len(total_withdupes))\n",
        "print(\"Duplicates to be removed for score 3 : \", duplicates_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwO3E3EV8rus",
        "outputId": "f0ee90a2-73ed-4258-b04c-ecb02da33560"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows including their duplicates(in Score 3):  1982\n",
            "Duplicates to be removed for score 3 :  991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re, unicodedata\n",
        "\n",
        "def clean_text(s):\n",
        "    s = str(s)\n",
        "    s = unicodedata.normalize(\"NFKC\", s)\n",
        "    s = s.replace(\"\\u00A0\", \" \")\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "finaldataset[\"EssayText\"] = finaldataset[\"EssayText\"].map(clean_text)\n",
        "\n",
        "# Split\n",
        "original = finaldataset[finaldataset[\"type\"] != \"paraphrased\"]     # keep all originals\n",
        "para = finaldataset[finaldataset[\"type\"] == \"paraphrased\"]\n",
        "\n",
        "# Remove duplicates only inside paraphrased rows and not dropping the original rows\n",
        "para = para.drop_duplicates(subset=[\"Score2\", \"EssayText\"])\n",
        "\n",
        "# Remove paraphrases versions that are exactly the same as an original row(a duplicate of the original row)\n",
        "orig_keys = set(zip(original[\"Score2\"], original[\"EssayText\"]))\n",
        "para = para[~para.apply(lambda r: (r[\"Score2\"], r[\"EssayText\"]) in orig_keys, axis=1)]\n",
        "\n",
        "finaldataset = pd.concat([original, para], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "Z20DLfCnBant"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finaldataset.to_csv(\"/content/drive/MyDrive/Data Augmentation/Dataset Splits/train_balanced_final.csv\",index=False)"
      ],
      "metadata": {
        "id": "4OYHw7eDDPxC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fd = pd.read_csv(\"/content/drive/MyDrive/Data Augmentation/Dataset Splits/train_balanced_final.csv\")\n",
        "print(\"rows after removing duplicate rows: \")\n",
        "print(fd[\"Score2\"].value_counts().sort_index())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec1Y8VsOBadd",
        "outputId": "70bd6cbe-85bb-4b39-8d0b-088256ec07b6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rows after removing duplicate rows: \n",
            "Score2\n",
            "0    5424\n",
            "1    4518\n",
            "2    4221\n",
            "3    3089\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(pd.read_csv(\"/content/drive/MyDrive/Data Augmentation/Dataset Splits/train_set.csv\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf63mBzFMfvl",
        "outputId": "7da12cd5-d7ae-42e0-d879-98b2f17b26ae"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fd_3 = fd[fd[\"Score2\"].isin([3])]\n",
        "\n",
        "duplicates_3 = fd_3.duplicated(subset=[\"Score2\", \"EssayText\"], keep=False).sum()\n",
        "print(\"Duplicates for score 3: \",duplicates_3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnTCozg6BQhV",
        "outputId": "e852a972-0129-40c4-b9c7-4fdda60a976a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicates for score 3:  0\n"
          ]
        }
      ]
    }
  ]
}